import MDPs: preexperiment, preepisode, prestep, poststep, postepisode, postexperiment

export SACLearner

mutable struct SACLearner{Tâ‚›<:AbstractFloat, Tâ‚<:AbstractFloat} <: AbstractHook
    Ï€::SACPolicy{Tâ‚›, Tâ‚}
    critics
    Î³::Float32
    Î±::Float32
    Ï::Float32
    min_explore_steps::Int
    train_interval::Int
    batch_size::Int

    s::Union{Vector{Tâ‚›}, Nothing}
    buff::CircularBuffer{Tuple{Vector{Tâ‚›}, Vector{Tâ‚}, Float64, Vector{Tâ‚›}, Bool}}
    criticsâ€²
    optim_actor::Adam
    optim_critics::Adam

    stats::Dict{Symbol, Float32}

    function SACLearner(Ï€::SACPolicy{Tâ‚›, Tâ‚}, critic, Î³::Real, Î±, Î·_actor, Î·_critic; polyak=0.995, min_explore_steps=10000, train_interval=1, batch_size=32, buffer_size=1000000) where {Tâ‚› <: AbstractFloat, Tâ‚ <: AbstractFloat}
        buff = CircularBuffer{Tuple{Vector{Tâ‚›}, Vector{Tâ‚}, Float64, Vector{Tâ‚›}, Bool}}(buffer_size)
        new{Tâ‚›, Tâ‚}(Ï€, (critic, deepcopy(critic)), Î³, Î±, polyak, min_explore_steps, train_interval, batch_size, nothing, buff, (deepcopy(critic), deepcopy(critic)), Adam(Î·_actor), Adam(Î·_critic), Dict{Symbol, Float32}())
    end
end

function prestep(sac::SACLearner; env::AbstractMDP, kwargs...)
    sac.s = copy(state(env))
end

function poststep(sac::SACLearner{Tâ‚›, Tâ‚}; env::AbstractMDP{Vector{Tâ‚›}, Vector{Tâ‚}}, steps::Int, returns, rng::AbstractRNG, kwargs...) where {Tâ‚›, Tâ‚}
    @unpack Ï€, critics, Î³, Î±, Ï, batch_size, s, criticsâ€² = sac

    a, r, sâ€², d = copy(action(env)), reward(env), copy(state(env)), in_absorbing_state(env)
    push!(sac.buff, (s, a, r, sâ€², d))

    if steps >= sac.min_explore_steps && steps % sac.train_interval == 0
        replay_batch = rand(rng, sac.buff, batch_size)
        ğ¬, ğš, ğ«, ğ¬â€², ğ = map(i -> reduce((ğ±, y) -> cat(ğ±, y; dims=ndims(y) + 1), map(experience -> experience[i], replay_batch)), 1:5)
        ğ¬, ğš, ğ«, ğ¬â€², ğ = tof32.((ğ¬, ğš, ğ«, ğ¬â€², ğ))

        ğšâ€², logÏ€ğšâ€² = sample_action_logÏ€(Ï€, rng, ğ¬â€²)
        ğªâ€² = min.(map(critic -> critic(vcat(ğ¬â€², ğšâ€²))[1, :], criticsâ€²)...)
        ğ¯â€² = ğªâ€² - Î± * logÏ€ğšâ€²
        ğª = ğ« + (1 .- ğ) * Î³ .* ğ¯â€²
        Ï• = Flux.params(critics...)
        â„“Ï•, âˆ‡Ï•â„“ = Flux.Zygote.withgradient(Ï•) do
            return sum(map(critic -> Flux.mse(critic(vcat(ğ¬, ğš))[1, :], ğª), critics)) / length(critics)
        end
        Flux.update!(sac.optim_critics, Ï•, âˆ‡Ï•â„“)

        Î¸ = Flux.params(Ï€.actor_model)
        â„“Î¸, âˆ‡Î¸â„“ = Flux.Zygote.withgradient(Î¸) do
            ğš, logÏ€ğš = sample_action_logÏ€(Ï€, rng, ğ¬)
            ğª = min.(map(critic -> critic(vcat(ğ¬, ğš))[1, :], critics)...)
            ğ¯ = ğª - Î± * logÏ€ğš
            return -mean(ğ¯)
        end
        Flux.update!(sac.optim_actor, Î¸, âˆ‡Î¸â„“)

        H = -mean(sample_action_logÏ€(Ï€, rng, ğ¬)[2])
        target_ent::Float32 = -1 / size(action_space(env), 1)
        Î± = clamp(exp(log(Î±) - 0.0003f0 * (H - target_ent)), 0.0001f0, 1000f0)
        sac.Î± = Î±

        Ï•â€² = Flux.params(criticsâ€²)
        for (param, paramâ€²) in zip(Ï•, Ï•â€²); copy!(paramâ€², Ï * paramâ€² + (1 - Ï) * param); end

        if steps % 1000 == 0
            vÌ„ = -â„“Î¸
            sac.stats[:â„“Ï•] = â„“Ï•
            sac.stats[:â„“Î¸] = â„“Î¸
            sac.stats[:vÌ„] = vÌ„
            sac.stats[:H] = H
            sac.stats[:Î±] = Î±
            episodes = length(returns)
            @debug "learning stats" steps episodes â„“Ï• â„“Î¸ vÌ„ H Î±
        end
    end
    nothing
end
