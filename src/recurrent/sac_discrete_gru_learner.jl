import MDPs: preexperiment, preepisode, prestep, poststep, postepisode, postexperiment
using Random
export RecurrentSACDiscreteLearner

mutable struct RecurrentSACDiscreteLearner{T<:AbstractFloat} <: AbstractHook
    Ï€::ContextualSACDiscretePolicy{T}
    critics
    critic_crnn
    Î³::Float32
    Î±::Float32
    Ï::Float32
    min_explore_steps::Int
    batch_size::Int
    horizon::Int
    tbptt_horizon::Int
    auto_tune_Î±::Bool
    device

    buff::AbstractArray{Float32, 2}  # sequence of evidence
    buff_head::Int
    traj_start_points::Set{Int}
    minibatch                               # preallocated memory for sampling a minibatch. Tuple ğ, ğ¨, ğš, ğ«, ğ¨â€², ğâ€², ğ§â€²
    ğœáµƒ::AbstractArray{Float32, 3}           # preallocated memory for recording actor's context during a rollout
    ğœá¶œ::AbstractArray{Float32, 3}           # preallocated memory for recording actor's context during a rollout

    actor::SACDiscretePolicy{T}     # train this sac actor and periodically copy weights to the original actor contextual policy.
    actor_crnn::GRUContextRNN               # train this actor crnn and periodically copy weights to the original actor context rnn
    criticsâ€²                                # target critic
    optim_actor::Adam
    optim_critics::Adam

    stats::Dict{Symbol, Float32}

    function RecurrentSACDiscreteLearner(Ï€::ContextualSACDiscretePolicy{T}, critic, critic_context_rnn, Î³::Real, horizon::Int, aspace::MDPs.IntegerSpace, sspace; Î±=0.1, Î·_actor=0.0003, Î·_critic=0.0003, polyak=0.995, batch_size=32, min_explore_steps=horizon*batch_size, tbptt_horizon=horizon, buffer_size=10000000, buff_mem_MB_cap=Inf, auto_tune_Î±=false, device=Flux.cpu) where {T <: AbstractFloat}
        each_entry_size = 1 + length(aspace) + 1 + size(sspace, 1) + 1
        buffer_size = min(buffer_size, buff_mem_MB_cap * 2^20 / (4 * each_entry_size)) |> floor |> Int
        buff = zeros(Float32, each_entry_size, buffer_size)
        ğ = zeros(Float32, each_entry_size, horizon + 1, batch_size) |> device
        ğ¨ = zeros(Float32, size(sspace, 1), horizon, batch_size) |> device
        ğš = zeros(Float32, size(aspace, 1), horizon, batch_size) |> device
        ğ« = zeros(Float32, horizon, batch_size) |> device
        ğ¨â€² = zeros(Float32, size(sspace, 1), horizon, batch_size) |> device
        ğâ€² = zeros(Float32, horizon, batch_size) |> device
        ğ§â€² = zeros(Float32, horizon, batch_size) |> device
        minibatch = (ğ, ğ¨, ğš, ğ«, ğ¨â€², ğâ€², ğ§â€²)
        ğœáµƒ = zeros(Float32, size(get_rnn_state(Ï€.crnn), 1), horizon + 1, batch_size) |> device
        ğœá¶œ = zeros(Float32, size(get_rnn_state(critic_context_rnn), 1), horizon + 1, batch_size) |> device
        new{T}(Ï€, (device(deepcopy(critic)), device(deepcopy(critic))), device(deepcopy(critic_context_rnn)), Î³, Î±, polyak, min_explore_steps, batch_size, horizon, tbptt_horizon, auto_tune_Î±, device, buff, 1, Set{Int}(), minibatch, ğœáµƒ, ğœá¶œ, device(deepcopy(Ï€.Ï€)), device(deepcopy(Ï€.crnn)), (device(deepcopy(critic)), device(deepcopy(critic))), Adam(Î·_actor), Adam(Î·_critic), Dict{Symbol, Float32}())
    end
end

function increment_buff_head!(sac::RecurrentSACDiscreteLearner)
    cap = size(sac.buff, 2)
    sac.buff_head = ((sac.buff_head + 1) - 1) % cap + 1
    nothing
end

function push_to_buff!(sac::RecurrentSACDiscreteLearner, is_new_traj, prev_action::Int, prev_reward, cur_state, cur_state_terminal, aspace::MDPs.IntegerSpace)
    buff = sac.buff
    m = sac.buff_head
    n_actions = length(aspace)

    if Bool(buff[1, m])
        delete!(sac.traj_start_points, m)
    end

    buff[1, m] = is_new_traj
    buff[1+1:1+n_actions, m] .= 0f0
    buff[1+prev_action, m] = 1f0
    buff[1+n_actions+1, m] = prev_reward
    buff[1+n_actions+1+1:1+n_actions+1+length(cur_state), m] .= cur_state
    buff[end, m] = cur_state_terminal

    if Bool(is_new_traj)
        push!(sac.traj_start_points, m)
    end

    increment_buff_head!(sac)
    nothing
end

function sample_from_buff!(sac::RecurrentSACDiscreteLearner, env)
    (ğ, ğ¨, ğš, ğ«, ğ¨â€², ğâ€², ğ§â€²), seq_len, batch_size = sac.minibatch, sac.horizon + 1, sac.batch_size
    cap = size(sac.buff, 2)
    @assert seq_len < cap
    n_actions = length(action_space(env))

    
    function isvalidstartpoint(p_begin)
        """Ensures that the buffer head doesn't lie in between any sampled trajectory:"""
        p_end = ((p_begin + seq_len - 1) - 1) % cap + 1
        if p_end > p_begin
            return !(p_begin <= sac.buff_head <= p_end)
        else
            return p_end < sac.buff_head < p_begin
        end
    end

    for n in 1:batch_size
        start_index = rand(sac.traj_start_points)
        while !isvalidstartpoint(start_index)
            start_index = rand(sac.traj_start_points)
        end
        indices = ((start_index:(start_index .+ seq_len - 1)) .- 1) .% cap .+ 1
        ğ[:, :, n] .= sac.device(sac.buff[:, indices])
    end

    # Note: "actions" are onehot
    prev_actions = @view ğ[1+1:1+n_actions, :, :]
    prev_rewards = @view ğ[1+n_actions+1, :, :]
    cur_obs = @view ğ[1+n_actions+1+1:1+n_actions+1+length(state(env)), :, :]
    
    obs = @view cur_obs[:, 1:end-1, :]
    actions = @view prev_actions[:, 2:end, :]
    rewards = @view prev_rewards[2:end, :]
    next_obs = @view cur_obs[:, 2:end, :]
    next_isterminals = @view ğ[end, 2:end, :]
    next_is_newtrajs = @view ğ[1, 2:end, :]

    copy!(ğ¨, obs); copy!(ğš, actions); copy!(ğ«, rewards); copy!(ğ¨â€², next_obs); copy!(ğâ€², next_isterminals); copy!(ğ§â€², next_is_newtrajs)

    return ğ, ğ¨, ğš, ğ«, ğ¨â€², ğâ€², ğ§â€²
end

function preepisode(sac::RecurrentSACDiscreteLearner; env, kwargs...)
    push_to_buff!(sac, true, 1, 0f0, state(env), in_absorbing_state(env), action_space(env))
end

function poststep(sac::RecurrentSACDiscreteLearner{T}; env::AbstractMDP{Vector{T}, Int}, steps::Int, returns, rng::AbstractRNG, kwargs...) where {T}
    @unpack actor, actor_crnn, critics, critic_crnn, Î³, Î±, Ï, batch_size, horizon, tbptt_horizon, ğœá¶œ, ğœáµƒ, criticsâ€² = sac

    push_to_buff!(sac, false, action(env), reward(env), state(env), in_absorbing_state(env), action_space(env))

    if steps >= sac.min_explore_steps && (steps % 50 == 0)
        @debug "sampling trajectories"
        ğ, ğ¨, ğš, ğ«, ğ¨â€², ğâ€², ğ§â€²  = sample_from_buff!(sac, env)
        # note: ğš is onehot!

        function actor_update()
            Î¸ = Flux.params(actor, actor_crnn)
            Flux.reset!.((actor_crnn, critic_crnn))
            fill!(ğœá¶œ, 0f0)
            for t in 1:horizon
                ğœá¶œ[:, t, :] .= @views critic_crnn(ğ[:, t, :])
            end
            ğ¬á¶œ = @views reshape(vcat(ğœá¶œ[:, 1:horizon, :], ğ¨), :, horizon * batch_size)
            ğ¨ = reshape(ğ¨, :, horizon * batch_size)
            entropy = 0f0
            â„“Î¸, âˆ‡Î¸â„“ = withgradient(Î¸) do
                _ğœáµƒ = reduce(hcat, map(1:horizon) do t
                    @views reshape(actor_crnn(ğ[:, t, :]), :, 1, batch_size)
                end)
                _ğœáµƒ = reshape(_ğœáµƒ, :, horizon * batch_size)
                ğ¬áµƒ = vcat(_ğœáµƒ, ğ¨)
                ğ›‘, logğ›‘ = get_probs_logprobs(actor, ğ¬áµƒ)
                ğªÌ‚ = min.(map(critic -> critic(ğ¬á¶œ), critics)...)
                ğ¯Ì‚ = sum(ğ›‘ .* (ğªÌ‚ - Î± * logğ›‘); dims=1)
                entropy += -mean(sum(ğ›‘ .* logğ›‘; dims=1))
                â„“Î¸ = -mean(ğ¯Ì‚)
                return â„“Î¸
            end
            Flux.update!(sac.optim_actor, Î¸, âˆ‡Î¸â„“)
            return â„“Î¸, entropy 
        end

        function critic_update()
            Ï• = Flux.params(critics..., critic_crnn)
            Flux.reset!.((actor_crnn, critic_crnn))
            fill!(ğœáµƒ, 0f0)
            fill!(ğœá¶œ, 0f0)
            for t in 1:(horizon+1)
                ğœáµƒ[:, t, :] .= @views actor_crnn(ğ[:, t, :])
                ğœá¶œ[:, t, :] .= @views critic_crnn(ğ[:, t, :])
            end
            ğœáµƒâ€² = @view ğœáµƒ[:, 2:end, :]
            ğœá¶œâ€² = @view ğœá¶œ[:, 2:end, :]
            ğ¬áµƒâ€² = reshape(vcat(ğœáµƒâ€², ğ¨â€²), :, horizon * batch_size)
            ğ¬á¶œâ€² = reshape(vcat(ğœá¶œâ€², ğ¨â€²), :, horizon * batch_size)

            ğ›‘â€², logğ›‘â€² = get_probs_logprobs(actor, ğ¬áµƒâ€²)
            ğªÌ‚â€² = min.(map(critic -> critic(ğ¬á¶œâ€²), criticsâ€²)...)
            ğ¯Ì‚â€² =  sum(ğ›‘â€² .* (ğªÌ‚â€² - Î± * logğ›‘â€²); dims=1)[1, :]

            ğ¨ = reshape(ğ¨, :, horizon * batch_size)
            ğš = argmax(reshape(ğš, :, horizon * batch_size), dims=1)[1, :] # CartesianIndices
            ğ« = reshape(ğ«, horizon * batch_size)
            ğâ€² = reshape(ğâ€², horizon * batch_size)
            ğ§â€² = reshape(ğ§â€², horizon * batch_size)
            Flux.reset!.((actor_crnn, critic_crnn))
            â„“Ï•, âˆ‡Ï•â„“ = withgradient(Ï•) do
                _ğœá¶œ = reduce(hcat, map(1:horizon) do t
                    @views reshape(critic_crnn(ğ[:, t, :]), :, 1, batch_size)
                end)
                _ğœá¶œ = reshape(_ğœá¶œ, :, horizon * batch_size)
                ğ¬á¶œ = vcat(_ğœá¶œ, ğ¨)
                ğªÌ‚Â¹, ğªÌ‚Â² = critics[1](ğ¬á¶œ), critics[2](ğ¬á¶œ)
                ğ›…Â¹ = (ğ« + Î³ * (1f0 .- ğâ€²) .* ğ¯Ì‚â€² - ğªÌ‚Â¹[ğš]) .* (1f0 .- ğ§â€²)
                ğ›…Â² = (ğ« + Î³ * (1f0 .- ğâ€²) .* ğ¯Ì‚â€² - ğªÌ‚Â²[ğš]) .* (1f0 .- ğ§â€²)
                â„“Ï• = 0.5f0 * (mean(ğ›…Â¹.^2) + mean(ğ›…Â².^2))
                return â„“Ï•
            end
            Flux.update!(sac.optim_critics, Ï•, âˆ‡Ï•â„“)
            return â„“Ï•
        end

        function alpha_update(current_ent)
            target_ent::Float32 = 0.98f0 * log(length(action_space(env)))
            Î± = clamp(exp(log(Î±) - 0.0003f0 * (current_ent - target_ent)), 0.0001f0, 1000f0)
            sac.Î± = Î±
        end

        function target_critics_update()
            Ï• = Flux.params(sac.critics)
            Ï•â€² = Flux.params(sac.criticsâ€²)
            for (param, paramâ€²) in zip(Ï•, Ï•â€²)
                copy!(paramâ€², Ï * paramâ€² + (1 - Ï) * param)
            end
        end

        function copy_back_actor_params()
            Î¸ = Flux.params(actor, actor_crnn)
            Î¸â€² = Flux.params(sac.Ï€.Ï€, sac.Ï€.crnn)
            for (param, paramâ€²) in zip(Î¸, Î¸â€²)
                copy!(paramâ€², param)
            end
        end

        @debug "actor update"
        â„“Î¸, H = actor_update()

        @debug "critic update"
        â„“Ï• = critic_update()
        
        if sac.auto_tune_Î±
            @debug "temperature update"
            Î± = alpha_update(H)
        end

        @debug "target critics update"
        target_critics_update()

        @debug "actor parameters back to the actor"
        copy_back_actor_params()


        vÌ„ = -â„“Î¸
        sac.stats[:â„“Ï•] = â„“Ï•
        sac.stats[:â„“Î¸] = â„“Î¸
        sac.stats[:vÌ„] = vÌ„
        sac.stats[:H] = H
        sac.stats[:Î±] = Î±
        episodes = length(returns)
        @debug "learning stats" steps episodes â„“Ï• â„“Î¸ vÌ„ H Î±
    end
    nothing
end
