import MDPs: preexperiment, preepisode, prestep, poststep, postepisode, postexperiment

export SACDiscreteLearner

mutable struct SACDiscreteLearner{T<:AbstractFloat} <: AbstractHook
    Ï€::SACDiscretePolicy{T}
    critics
    Î³::Float32
    Î±::Float32
    Ï::Float32
    min_explore_steps::Int
    train_interval::Int
    batch_size::Int
    auto_tune_Î±::Bool

    s::Union{Vector{T}, Nothing}
    buff::CircularBuffer{Tuple{Vector{T}, Int, Float64, Vector{T}, Bool}}
    criticsâ€²
    optim_actor::Adam
    optim_critics::Adam

    stats::Dict{Symbol, Float32}

    function SACDiscreteLearner(Ï€::SACDiscretePolicy{T}, critic, Î³::Real; Î±=0.1, Î·_actor=0.0003, Î·_critic=0.0003, polyak=0.995, min_explore_steps=10000, train_interval=1, batch_size=64, buffer_size=1000000, auto_tune_Î±=false) where {T <: AbstractFloat}
        buff = CircularBuffer{Tuple{Vector{T}, Int, Float64, Vector{T}, Bool}}(buffer_size)
        new{T}(Ï€, (critic, deepcopy(critic)), Î³, Î±, polyak, min_explore_steps, train_interval, batch_size, auto_tune_Î±, nothing, buff, (deepcopy(critic), deepcopy(critic)), Adam(Î·_actor), Adam(Î·_critic), Dict{Symbol, Float32}())
    end
end


function prestep(sac::SACDiscreteLearner; env::AbstractMDP, kwargs...)
    sac.s = copy(state(env))
end

function poststep(sac::SACDiscreteLearner{T}; env::AbstractMDP{Vector{T}, Int}, steps::Int, returns, rng::AbstractRNG, kwargs...) where {T}
    @unpack Ï€, critics, Î³, Î±, Ï, batch_size, s, criticsâ€² = sac

    a, r, sâ€², d = copy(action(env)), reward(env), copy(state(env)), in_absorbing_state(env)
    push!(sac.buff, (s, a, r, sâ€², d))

    if steps >= sac.min_explore_steps && steps % sac.train_interval == 0
        replay_batch = rand(rng, sac.buff, batch_size)
        ğ¬, ğš, ğ«, ğ¬â€², ğ = map(i -> reduce((ğ±, y) -> cat(ğ±, y; dims=ndims(y) + 1), map(experience -> experience[i], replay_batch)), 1:5)
        ğ¬, ğ«, ğ¬â€², ğ = tof32.((ğ¬, ğ«, ğ¬â€², ğ))
        ğš = map(i -> CartesianIndex(ğš[i], i), 1:batch_size)

        ğ›‘â€², logğ›‘â€² = get_probs_logprobs(Ï€, ğ¬â€²)
        ğªÌ‚â€² = min.(map(critic -> critic(ğ¬â€²), criticsâ€²)...)
        ğ¯Ì‚â€² =  sum(ğ›‘â€² .* (ğªÌ‚â€² - Î± * logğ›‘â€²); dims=1)[1, :]
    
        Ï• = Flux.params(critics...)
        â„“Ï•, âˆ‡Ï•â„“ = Flux.Zygote.withgradient(Ï•) do
            ğªÌ‚Â¹, ğªÌ‚Â² = critics[1](ğ¬), critics[2](ğ¬)
            println(size.((ğ«, ğâ€², ğ¯Ì‚â€², ğš, ğªÌ‚Â¹, ğªÌ‚Â¹[ğš])))
            ğ›…Â¹ = (ğ« + Î³ * (1f0 .- ğ) .* ğ¯Ì‚â€² - ğªÌ‚Â¹[ğš])
            ğ›…Â² = (ğ« + Î³ * (1f0 .- ğ) .* ğ¯Ì‚â€² - ğªÌ‚Â²[ğš])
            â„“Ï• = 0.5f0 * (mean(ğ›…Â¹.^2) + mean(ğ›…Â².^2))
            return â„“Ï• 
        end
        Flux.update!(sac.optim_critics, Ï•, âˆ‡Ï•â„“)

        Î¸ = Flux.params(Ï€)
        â„“Î¸, âˆ‡Î¸â„“ = Flux.Zygote.withgradient(Î¸) do
            ğ›‘, logğ›‘ = get_probs_logprobs(Ï€, ğ¬)
            ğªÌ‚ = min.(map(critic -> critic(ğ¬), critics)...)
            ğ¯Ì‚ = sum(ğ›‘ .* (ğªÌ‚ - Î± * logğ›‘); dims=1)
            return -mean(ğ¯Ì‚)
        end
        Flux.update!(sac.optim_actor, Î¸, âˆ‡Î¸â„“)

        ğ›‘, logğ›‘ = get_probs_logprobs(Ï€, ğ¬)
        H = -mean(sum(ğ›‘ .* logğ›‘; dims=1))
        if sac.auto_tune_Î±
            target_ent::Float32 = 0.98f0 * log(length(action_space(env)))
            Î± = clamp(exp(log(Î±) - 0.0003f0 * (H - target_ent)), 0.0001f0, 1000f0)
            sac.Î± = Î±
        end
        
        Ï•â€² = Flux.params(criticsâ€²...)
        for (param, paramâ€²) in zip(Ï•, Ï•â€²); copy!(paramâ€², Ï * paramâ€² + (1 - Ï) * param); end

        if steps % 1000 == 0
            vÌ„ = -â„“Î¸
            sac.stats[:â„“Ï•] = â„“Ï•
            sac.stats[:â„“Î¸] = â„“Î¸
            sac.stats[:vÌ„] = vÌ„
            sac.stats[:H] = H
            sac.stats[:Î±] = Î±
            episodes = length(returns)
            @debug "learning stats" steps episodes â„“Ï• â„“Î¸ vÌ„ H Î±
        end
    end
    nothing
end
